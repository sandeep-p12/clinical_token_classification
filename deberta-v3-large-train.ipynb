{"cells":[{"cell_type":"markdown","metadata":{"id":"CxYm9TU57-zo"},"source":["# DeBERTa-v3 Large - 0.883 LB!"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"O0jMi6ol8Oom","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1649219692940,"user_tz":-330,"elapsed":14589,"user":{"displayName":"Sandeep pandey","userId":"05472944172049299897"}},"outputId":"6257bdcf-c49c-4cd1-9ab5-d2db4cb757ec"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting transformers\n","  Downloading transformers-4.17.0-py3-none-any.whl (3.8 MB)\n","\u001b[K     |████████████████████████████████| 3.8 MB 5.0 MB/s \n","\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Collecting sacremoses\n","  Downloading sacremoses-0.0.49-py3-none-any.whl (895 kB)\n","\u001b[K     |████████████████████████████████| 895 kB 52.6 MB/s \n","\u001b[?25hCollecting tokenizers!=0.11.3,>=0.11.1\n","  Downloading tokenizers-0.11.6-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.5 MB)\n","\u001b[K     |████████████████████████████████| 6.5 MB 55.9 MB/s \n","\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Collecting pyyaml>=5.1\n","  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n","\u001b[K     |████████████████████████████████| 596 kB 56.7 MB/s \n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.6.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.5)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.63.0)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.3)\n","Collecting huggingface-hub<1.0,>=0.1.0\n","  Downloading huggingface_hub-0.5.0-py3-none-any.whl (77 kB)\n","\u001b[K     |████████████████████████████████| 77 kB 8.0 MB/s \n","\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.7)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.7.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n","Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n","  Attempting uninstall: pyyaml\n","    Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","Successfully installed huggingface-hub-0.5.0 pyyaml-6.0 sacremoses-0.0.49 tokenizers-0.11.6 transformers-4.17.0\n","Collecting sentencepiece\n","  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n","\u001b[K     |████████████████████████████████| 1.2 MB 5.0 MB/s \n","\u001b[?25hInstalling collected packages: sentencepiece\n","Successfully installed sentencepiece-0.1.96\n"]}],"source":["!pip install transformers\n","!pip install sentencepiece"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"uf-08y_u84oA","executionInfo":{"status":"ok","timestamp":1649219692941,"user_tz":-330,"elapsed":8,"user":{"displayName":"Sandeep pandey","userId":"05472944172049299897"}}},"outputs":[],"source":["root_dir = '/gdrive/My Drive/NBME/'"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18615,"status":"ok","timestamp":1649219711550,"user":{"displayName":"Sandeep pandey","userId":"05472944172049299897"},"user_tz":-330},"id":"YiTI_w-x853v","outputId":"d0958272-57da-4c3c-c420-a23b9cfc8a79"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /gdrive\n"]}],"source":["from google.colab import drive\n","drive.mount('/gdrive')"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"83cKl8Xy7-zr","executionInfo":{"status":"ok","timestamp":1649219737522,"user_tz":-330,"elapsed":382,"user":{"displayName":"Sandeep pandey","userId":"05472944172049299897"}}},"outputs":[],"source":["# The following is necessary if you want to use the fast tokenizer for deberta v2 or v3\n","# This must be done before importing transformers\n","import shutil\n","from pathlib import Path\n","\n","transformers_path = Path(\"/usr/local/lib/python3.7/dist-packages/transformers\")\n","\n","convert_file = \"convert_slow_tokenizer.py\"\n","conversion_path = transformers_path/convert_file\n","\n","if conversion_path.exists():\n","    conversion_path.unlink()\n","\n","shutil.copy(convert_file, transformers_path)\n","deberta_v2_path = transformers_path / \"models\" / \"deberta_v2\"\n","\n","for filename in ['tokenization_deberta_v2.py', 'tokenization_deberta_v2_fast.py']:\n","    filepath = deberta_v2_path/filename\n","    \n","    if filepath.exists():\n","        filepath.unlink()\n","\n","    shutil.copy(filename, filepath)"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8255,"status":"ok","timestamp":1649219746428,"user":{"displayName":"Sandeep pandey","userId":"05472944172049299897"},"user_tz":-330},"id":"Y9CiZA4x7-zs","outputId":"734cbd5d-bbce-4415-9a55-af1d7766f87d"},"outputs":[{"output_type":"stream","name":"stdout","text":["env: TOKENIZERS_PARALLELISM=true\n"]}],"source":["import os\n","import gc\n","import re\n","import ast\n","import sys\n","import copy\n","import json\n","import time\n","import math\n","import string\n","import pickle\n","import random\n","import joblib\n","import itertools\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","import scipy as sp\n","import numpy as np\n","import pandas as pd\n","pd.set_option('display.max_rows', 500)\n","pd.set_option('display.max_columns', 500)\n","pd.set_option('display.width', 1000)\n","from tqdm.auto import tqdm\n","from sklearn.metrics import f1_score\n","from sklearn.model_selection import StratifiedKFold, GroupKFold, KFold\n","\n","import torch\n","import torch.nn as nn\n","from torch.nn import Parameter\n","import torch.nn.functional as F\n","from torch.optim import Adam, SGD, AdamW\n","from torch.utils.data import DataLoader, Dataset\n","\n","from transformers import AutoTokenizer, AutoModel, AutoConfig\n","from transformers import get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup\n","%env TOKENIZERS_PARALLELISM=true\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"JcKG8Rxs7-zt","executionInfo":{"status":"ok","timestamp":1649219746429,"user_tz":-330,"elapsed":4,"user":{"displayName":"Sandeep pandey","userId":"05472944172049299897"}}},"outputs":[],"source":["class CFG:\n","    debug=True\n","    apex=True\n","    print_freq=100\n","    num_workers=4\n","    model=\"roberta-large\"\n","    scheduler='cosine' # ['linear', 'cosine']\n","    batch_scheduler=True\n","    num_cycles=0.5\n","    num_warmup_steps=0\n","    epochs=5\n","    encoder_lr=2e-5\n","    decoder_lr=2e-5\n","    min_lr=1e-6\n","    eps=1e-6\n","    betas=(0.9, 0.999)\n","    batch_size=4\n","    fc_dropout=0.2\n","    max_len=512\n","    weight_decay=0.01\n","    gradient_accumulation_steps=1\n","    max_grad_norm=1000\n","    seed=42\n","    n_fold=5\n","    trn_fold=[0, 1, 2, 3, 4]\n","    train=True"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4YypsiZi7-zu"},"outputs":[],"source":["# train = pd.read_csv('../input/train-modified/train_modified.csv')\n","# train.drop(columns = ['Unnamed: 0', 'id'], inplace = True)\n","# train.drop_duplicates(keep='first', inplace = True)\n","# def location_gen(Series):\n","#   data = Series.tolist()\n","#   return data\n","# train = train.groupby(['pn_num', 'pn_history', 'feature_text']).agg({'annotation':location_gen, 'location':location_gen})\n","# train.reset_index(inplace = True)\n","# oldtrain = pd.read_csv('../input/nbme-score-clinical-patient-notes/train.csv')\n","# features = pd.read_csv('../input/nbme-score-clinical-patient-notes/features.csv')\n","# def preprocess_features(features):\n","#     features.loc[27, 'feature_text'] = \"Last-Pap-smear-1-year-ago\"\n","#     return features\n","# features = preprocess_features(features)\n","# case_no = oldtrain.groupby(['pn_num']).agg({'case_num':'first'})\n","# case_no.reset_index(inplace = True)\n","# train = train.merge(case_no, on=['pn_num'], how='left')\n","# train = train.merge(features, on=['case_num', 'feature_text'], how='left')\n","\n","# patient_notes = pd.read_csv('../input/nbme-score-clinical-patient-notes/patient_notes.csv')\n","# K = 5\n","# SEED = 2222\n","\n","# skf = StratifiedKFold(n_splits=K, random_state=SEED, shuffle=True)\n","# splits = list(skf.split(X=patient_notes, y=patient_notes['case_num']))\n","# folds = np.zeros(len(patient_notes), dtype=int)\n","# for i, (train_idx, val_idx) in enumerate(splits):\n","#     folds[val_idx] = i\n","#     df_val = patient_notes.iloc[val_idx]\n","# patient_notes['fold'] = folds\n","# train = train.merge(patient_notes, on=['pn_num', 'case_num', 'pn_history'], how='left')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lSEyMw747-zu"},"outputs":[],"source":["# train = pd.read_csv(root_dir+'dataset/fold_train.csv')\n","# train.drop(columns = ['Unnamed: 0'], inplace = True)\n","# train['annotation'] = train['annotation'].apply(ast.literal_eval)\n","# train['location'] = train['location'].apply(ast.literal_eval)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kVue2DXCxhFW"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EHv4zYR8JFwS"},"outputs":[],"source":["# train = pd.read_csv(root_dir+'dataset/train.csv')\n","# train['annotation'] = train['annotation'].apply(ast.literal_eval)\n","# train['location'] = train['location'].apply(ast.literal_eval)\n","\n","# features = pd.read_csv(root_dir+'dataset/features.csv')\n","# def preprocess_features(features):\n","#     features.loc[27, 'feature_text'] = \"Last-Pap-smear-1-year-ago\"\n","#     return features\n","# features = preprocess_features(features)\n","# patient_notes = pd.read_csv(root_dir+'dataset/patient_notes.csv')\n","# train = train.merge(features, on=['feature_num', 'case_num'], how='left')\n","# train = train.merge(patient_notes, on=['pn_num', 'case_num'], how='left')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f_Dso-H7Llb7"},"outputs":[],"source":["# train.loc[338, 'annotation'] = ast.literal_eval('[[\"father heart attack\"]]')\n","# train.loc[338, 'location'] = ast.literal_eval('[[\"764 783\"]]')\n","\n","# train.loc[621, 'annotation'] = ast.literal_eval('[[\"for the last 2-3 months\"]]')\n","# train.loc[621, 'location'] = ast.literal_eval('[[\"77 100\"]]')\n","\n","# train.loc[655, 'annotation'] = ast.literal_eval('[[\"no heat intolerance\"], [\"no cold intolerance\"]]')\n","# train.loc[655, 'location'] = ast.literal_eval('[[\"285 292;301 312\"], [\"285 287;296 312\"]]')\n","\n","# train.loc[1262, 'annotation'] = ast.literal_eval('[[\"mother thyroid problem\"]]')\n","# train.loc[1262, 'location'] = ast.literal_eval('[[\"551 557;565 580\"]]')\n","\n","# train.loc[1265, 'annotation'] = ast.literal_eval('[[\\'felt like he was going to \"pass out\"\\']]')\n","# train.loc[1265, 'location'] = ast.literal_eval('[[\"131 135;181 212\"]]')\n","\n","# train.loc[1396, 'annotation'] = ast.literal_eval('[[\"stool , with no blood\"]]')\n","# train.loc[1396, 'location'] = ast.literal_eval('[[\"259 280\"]]')\n","\n","# train.loc[1591, 'annotation'] = ast.literal_eval('[[\"diarrhoe non blooody\"]]')\n","# train.loc[1591, 'location'] = ast.literal_eval('[[\"176 184;201 212\"]]')\n","\n","# train.loc[1615, 'annotation'] = ast.literal_eval('[[\"diarrhea for last 2-3 days\"]]')\n","# train.loc[1615, 'location'] = ast.literal_eval('[[\"249 257;271 288\"]]')\n","\n","# train.loc[1664, 'annotation'] = ast.literal_eval('[[\"no vaginal discharge\"]]')\n","# train.loc[1664, 'location'] = ast.literal_eval('[[\"822 824;907 924\"]]')\n","\n","# train.loc[1714, 'annotation'] = ast.literal_eval('[[\"started about 8-10 hours ago\"]]')\n","# train.loc[1714, 'location'] = ast.literal_eval('[[\"101 129\"]]')\n","\n","# train.loc[1929, 'annotation'] = ast.literal_eval('[[\"no blood in the stool\"]]')\n","# train.loc[1929, 'location'] = ast.literal_eval('[[\"531 539;549 561\"]]')\n","\n","# train.loc[2134, 'annotation'] = ast.literal_eval('[[\"last sexually active 9 months ago\"]]')\n","# train.loc[2134, 'location'] = ast.literal_eval('[[\"540 560;581 593\"]]')\n","\n","# train.loc[2191, 'annotation'] = ast.literal_eval('[[\"right lower quadrant pain\"]]')\n","# train.loc[2191, 'location'] = ast.literal_eval('[[\"32 57\"]]')\n","\n","# train.loc[2553, 'annotation'] = ast.literal_eval('[[\"diarrhoea no blood\"]]')\n","# train.loc[2553, 'location'] = ast.literal_eval('[[\"308 317;376 384\"]]')\n","\n","# train.loc[3124, 'annotation'] = ast.literal_eval('[[\"sweating\"]]')\n","# train.loc[3124, 'location'] = ast.literal_eval('[[\"549 557\"]]')\n","\n","# train.loc[3858, 'annotation'] = ast.literal_eval('[[\"previously as regular\"], [\"previously eveyr 28-29 days\"], [\"previously lasting 5 days\"], [\"previously regular flow\"]]')\n","# train.loc[3858, 'location'] = ast.literal_eval('[[\"102 123\"], [\"102 112;125 141\"], [\"102 112;143 157\"], [\"102 112;159 171\"]]')\n","\n","# train.loc[4373, 'annotation'] = ast.literal_eval('[[\"for 2 months\"]]')\n","# train.loc[4373, 'location'] = ast.literal_eval('[[\"33 45\"]]')\n","\n","# train.loc[4763, 'annotation'] = ast.literal_eval('[[\"35 year old\"]]')\n","# train.loc[4763, 'location'] = ast.literal_eval('[[\"5 16\"]]')\n","\n","# train.loc[4782, 'annotation'] = ast.literal_eval('[[\"darker brown stools\"]]')\n","# train.loc[4782, 'location'] = ast.literal_eval('[[\"175 194\"]]')\n","\n","# train.loc[4908, 'annotation'] = ast.literal_eval('[[\"uncle with peptic ulcer\"]]')\n","# train.loc[4908, 'location'] = ast.literal_eval('[[\"700 723\"]]')\n","\n","# train.loc[6016, 'annotation'] = ast.literal_eval('[[\"difficulty falling asleep\"]]')\n","# train.loc[6016, 'location'] = ast.literal_eval('[[\"225 250\"]]')\n","\n","# train.loc[6192, 'annotation'] = ast.literal_eval('[[\"helps to take care of aging mother and in-laws\"]]')\n","# train.loc[6192, 'location'] = ast.literal_eval('[[\"197 218;236 260\"]]')\n","\n","# train.loc[6380, 'annotation'] = ast.literal_eval('[[\"No hair changes\"], [\"No skin changes\"], [\"No GI changes\"], [\"No palpitations\"], [\"No excessive sweating\"]]')\n","# train.loc[6380, 'location'] = ast.literal_eval('[[\"480 482;507 519\"], [\"480 482;499 503;512 519\"], [\"480 482;521 531\"], [\"480 482;533 545\"], [\"480 482;564 582\"]]')\n","\n","# train.loc[6562, 'annotation'] = ast.literal_eval('[[\"stressed due to taking care of her mother\"], [\"stressed due to taking care of husbands parents\"]]')\n","# train.loc[6562, 'location'] = ast.literal_eval('[[\"290 320;327 337\"], [\"290 320;342 358\"]]')\n","\n","# train.loc[6862, 'annotation'] = ast.literal_eval('[[\"stressor taking care of many sick family members\"]]')\n","# train.loc[6862, 'location'] = ast.literal_eval('[[\"288 296;324 363\"]]')\n","\n","# train.loc[7022, 'annotation'] = ast.literal_eval('[[\"heart started racing and felt numbness for the 1st time in her finger tips\"]]')\n","# train.loc[7022, 'location'] = ast.literal_eval('[[\"108 182\"]]')\n","\n","# train.loc[7422, 'annotation'] = ast.literal_eval('[[\"first started 5 yrs\"]]')\n","# train.loc[7422, 'location'] = ast.literal_eval('[[\"102 121\"]]')\n","\n","# train.loc[8876, 'annotation'] = ast.literal_eval('[[\"No shortness of breath\"]]')\n","# train.loc[8876, 'location'] = ast.literal_eval('[[\"481 483;533 552\"]]')\n","\n","# train.loc[9027, 'annotation'] = ast.literal_eval('[[\"recent URI\"], [\"nasal stuffines, rhinorrhea, for 3-4 days\"]]')\n","# train.loc[9027, 'location'] = ast.literal_eval('[[\"92 102\"], [\"123 164\"]]')\n","\n","# train.loc[9938, 'annotation'] = ast.literal_eval('[[\"irregularity with her cycles\"], [\"heavier bleeding\"], [\"changes her pad every couple hours\"]]')\n","# train.loc[9938, 'location'] = ast.literal_eval('[[\"89 117\"], [\"122 138\"], [\"368 402\"]]')\n","\n","# train.loc[9973, 'annotation'] = ast.literal_eval('[[\"gaining 10-15 lbs\"]]')\n","# train.loc[9973, 'location'] = ast.literal_eval('[[\"344 361\"]]')\n","\n","# train.loc[10513, 'annotation'] = ast.literal_eval('[[\"weight gain\"], [\"gain of 10-16lbs\"]]')\n","# train.loc[10513, 'location'] = ast.literal_eval('[[\"600 611\"], [\"607 623\"]]')\n","\n","# train.loc[11551, 'annotation'] = ast.literal_eval('[[\"seeing her son knows are not real\"]]')\n","# train.loc[11551, 'location'] = ast.literal_eval('[[\"386 400;443 461\"]]')\n","\n","# train.loc[11677, 'annotation'] = ast.literal_eval('[[\"saw him once in the kitchen after he died\"]]')\n","# train.loc[11677, 'location'] = ast.literal_eval('[[\"160 201\"]]')\n","\n","# train.loc[12124, 'annotation'] = ast.literal_eval('[[\"tried Ambien but it didnt work\"]]')\n","# train.loc[12124, 'location'] = ast.literal_eval('[[\"325 337;349 366\"]]')\n","\n","# train.loc[12279, 'annotation'] = ast.literal_eval('[[\"heard what she described as a party later than evening these things did not actually happen\"]]')\n","# train.loc[12279, 'location'] = ast.literal_eval('[[\"405 459;488 524\"]]')\n","\n","# train.loc[12289, 'annotation'] = ast.literal_eval('[[\"experienced seeing her son at the kitchen table these things did not actually happen\"]]')\n","# train.loc[12289, 'location'] = ast.literal_eval('[[\"353 400;488 524\"]]')\n","\n","# train.loc[13238, 'annotation'] = ast.literal_eval('[[\"SCRACHY THROAT\"], [\"RUNNY NOSE\"]]')\n","# train.loc[13238, 'location'] = ast.literal_eval('[[\"293 307\"], [\"321 331\"]]')\n","\n","# train.loc[13297, 'annotation'] = ast.literal_eval('[[\"without improvement when taking tylenol\"], [\"without improvement when taking ibuprofen\"]]')\n","# train.loc[13297, 'location'] = ast.literal_eval('[[\"182 221\"], [\"182 213;225 234\"]]')\n","\n","# train.loc[13299, 'annotation'] = ast.literal_eval('[[\"yesterday\"], [\"yesterday\"]]')\n","# train.loc[13299, 'location'] = ast.literal_eval('[[\"79 88\"], [\"409 418\"]]')\n","\n","# train.loc[13845, 'annotation'] = ast.literal_eval('[[\"headache global\"], [\"headache throughout her head\"]]')\n","# train.loc[13845, 'location'] = ast.literal_eval('[[\"86 94;230 236\"], [\"86 94;237 256\"]]')\n","\n","# train.loc[14083, 'annotation'] = ast.literal_eval('[[\"headache generalized in her head\"]]')\n","# train.loc[14083, 'location'] = ast.literal_eval('[[\"56 64;156 179\"]]')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZmsKJ1XaLmPK"},"outputs":[],"source":["# train['annotation_length'] = train['annotation'].apply(len)\n","# train = train[train['annotation_length'] == 0]\n","# train.reset_index(drop = True, inplace = True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jAuVb_xY0oKC"},"outputs":[],"source":["# K = 5\n","# SEED = 2222\n","\n","# skf = StratifiedKFold(n_splits=K, random_state=SEED, shuffle=True)\n","# splits = list(skf.split(X=patient_notes, y=patient_notes['case_num']))\n","# folds = np.zeros(len(patient_notes), dtype=int)\n","# for i, (train_idx, val_idx) in enumerate(splits):\n","#     folds[val_idx] = i\n","#     df_val = patient_notes.iloc[val_idx]\n","# patient_notes['fold'] = folds\n","# train = train.merge(patient_notes, on=['pn_num', 'case_num', 'pn_history'], how='left')"]},{"cell_type":"markdown","metadata":{"id":"1KcvdleA7-zv"},"source":["# Tokenizer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hjC1oFe4Gt_B"},"outputs":[],"source":["# clean = pd.read_csv(root_dir+'dataset/train_with_folds.csv')\n","# clean.drop(columns = ['Unnamed: 0'], inplace = True)\n","# clean['annotation'] = clean['annotation'].apply(ast.literal_eval)\n","# clean['location'] = clean['location'].apply(ast.literal_eval)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"S0mFCaLR25Rk"},"outputs":[],"source":["# train = pd.concat([clean, train])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2xWC53iotEb2"},"outputs":[],"source":["# train.to_csv(root_dir+'dataset/train_with_folds_not_clean.csv')"]},{"cell_type":"code","source":["features = pd.read_csv(root_dir+'dataset/features.csv')"],"metadata":{"id":"fqz16qOcLJ5j","executionInfo":{"status":"ok","timestamp":1649220037962,"user_tz":-330,"elapsed":661,"user":{"displayName":"Sandeep pandey","userId":"05472944172049299897"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["features.head(3)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":143},"id":"WdDU7hRHLLVk","executionInfo":{"status":"ok","timestamp":1649220113964,"user_tz":-330,"elapsed":7,"user":{"displayName":"Sandeep pandey","userId":"05472944172049299897"}},"outputId":"6bc28486-1386-4264-eb75-37ddc8783a8e"},"execution_count":13,"outputs":[{"output_type":"execute_result","data":{"text/plain":["   feature_num  case_num                                       feature_text\n","0            0         0  Family-history-of-MI-OR-Family-history-of-myoc...\n","1            1         0                 Family-history-of-thyroid-disorder\n","2            2         0                                     Chest-pressure"],"text/html":["\n","  <div id=\"df-b3b73cc9-87e6-4a8c-9ed2-830cc1d60dd1\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>feature_num</th>\n","      <th>case_num</th>\n","      <th>feature_text</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>Family-history-of-MI-OR-Family-history-of-myoc...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>Family-history-of-thyroid-disorder</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>Chest-pressure</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b3b73cc9-87e6-4a8c-9ed2-830cc1d60dd1')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-b3b73cc9-87e6-4a8c-9ed2-830cc1d60dd1 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-b3b73cc9-87e6-4a8c-9ed2-830cc1d60dd1');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":13}]},{"cell_type":"code","source":["patient_notes = pd.read_csv(root_dir+'dataset/patient_notes.csv')"],"metadata":{"id":"KkQ7UiL6KUnA","executionInfo":{"status":"ok","timestamp":1649219822719,"user_tz":-330,"elapsed":2432,"user":{"displayName":"Sandeep pandey","userId":"05472944172049299897"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["patient_notes.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wTngepknKXP6","executionInfo":{"status":"ok","timestamp":1649220068491,"user_tz":-330,"elapsed":4,"user":{"displayName":"Sandeep pandey","userId":"05472944172049299897"}},"outputId":"dc58072a-a38a-451b-c3c0-09a1935ac45b"},"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(42146, 3)"]},"metadata":{},"execution_count":11}]},{"cell_type":"code","source":["patient_notes = patient_notes.merge(features, on=['case_num'], how='left')"],"metadata":{"id":"BVCig_P4LYvd","executionInfo":{"status":"ok","timestamp":1649220125822,"user_tz":-330,"elapsed":1,"user":{"displayName":"Sandeep pandey","userId":"05472944172049299897"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["patient_notes.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"j9qMTd2GLhGP","executionInfo":{"status":"ok","timestamp":1649220132942,"user_tz":-330,"elapsed":3,"user":{"displayName":"Sandeep pandey","userId":"05472944172049299897"}},"outputId":"7524076a-6ffc-4f5a-b465-b425974b1218"},"execution_count":15,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(626902, 5)"]},"metadata":{},"execution_count":15}]},{"cell_type":"code","source":["train = pd.read_csv(root_dir+'dataset/train.csv')"],"metadata":{"id":"7izbczS0M228","executionInfo":{"status":"ok","timestamp":1649220485093,"user_tz":-330,"elapsed":361,"user":{"displayName":"Sandeep pandey","userId":"05472944172049299897"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["train.head(2)"],"metadata":{"id":"nYgZ0q-fM4XD","executionInfo":{"status":"ok","timestamp":1649220492018,"user_tz":-330,"elapsed":448,"user":{"displayName":"Sandeep pandey","userId":"05472944172049299897"}},"outputId":"a8a7836b-8e41-496e-8477-b3d6735449d4","colab":{"base_uri":"https://localhost:8080/","height":112}},"execution_count":17,"outputs":[{"output_type":"execute_result","data":{"text/plain":["          id  case_num  pn_num  feature_num                        annotation     location\n","0  00016_000         0      16            0  ['dad with recent heart attcak']  ['696 724']\n","1  00016_001         0      16            1     ['mom with \"thyroid disease']  ['668 693']"],"text/html":["\n","  <div id=\"df-42b21d82-7bd8-4811-b814-76fe724a0b12\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>case_num</th>\n","      <th>pn_num</th>\n","      <th>feature_num</th>\n","      <th>annotation</th>\n","      <th>location</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>00016_000</td>\n","      <td>0</td>\n","      <td>16</td>\n","      <td>0</td>\n","      <td>['dad with recent heart attcak']</td>\n","      <td>['696 724']</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>00016_001</td>\n","      <td>0</td>\n","      <td>16</td>\n","      <td>1</td>\n","      <td>['mom with \"thyroid disease']</td>\n","      <td>['668 693']</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-42b21d82-7bd8-4811-b814-76fe724a0b12')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-42b21d82-7bd8-4811-b814-76fe724a0b12 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-42b21d82-7bd8-4811-b814-76fe724a0b12');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":17}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZPoNZyM6zS86"},"outputs":[],"source":["train = pd.read_csv(root_dir+'dataset/train_with_folds_not_clean.csv')\n","train.drop(columns = ['Unnamed: 0'], inplace = True)\n","train['annotation'] = train['annotation'].apply(ast.literal_eval)\n","train['location'] = train['location'].apply(ast.literal_eval)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3TlezUKH7-zv"},"outputs":[],"source":["from transformers.models.deberta_v2.tokenization_deberta_v2_fast import DebertaV2TokenizerFast\n","\n","tokenizer = DebertaV2TokenizerFast.from_pretrained(root_dir+'deberta_tokenizer')\n","# tokenizer.save_pretrained('tokenizer/')\n","CFG.tokenizer = tokenizer"]},{"cell_type":"markdown","metadata":{"id":"d7TZagjG7-zw"},"source":["# Helper functions for scoring"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cHT4w7FS7-zw"},"outputs":[],"source":["# From https://www.kaggle.com/theoviel/evaluation-metric-folds-baseline\n","def micro_f1(preds, truths):\n","    \"\"\"\n","    Micro f1 on binary arrays.\n","\n","    Args:\n","        preds (list of lists of ints): Predictions.\n","        truths (list of lists of ints): Ground truths.\n","\n","    Returns:\n","        float: f1 score.\n","    \"\"\"\n","    # Micro : aggregating over all instances\n","    preds = np.concatenate(preds)\n","    truths = np.concatenate(truths)\n","    \n","    return f1_score(truths, preds)\n","\n","\n","def spans_to_binary(spans, length=None):\n","    \"\"\"\n","    Converts spans to a binary array indicating whether each character is in the span.\n","\n","    Args:\n","        spans (list of lists of two ints): Spans.\n","\n","    Returns:\n","        np array [length]: Binarized spans.\n","    \"\"\"\n","    length = np.max(spans) if length is None else length\n","    binary = np.zeros(length)\n","    for start, end in spans:\n","        binary[start:end] = 1\n","        \n","    return binary\n","\n","\n","def span_micro_f1(preds, truths):\n","    \"\"\"\n","    Micro f1 on spans.\n","\n","    Args:\n","        preds (list of lists of two ints): Prediction spans.\n","        truths (list of lists of two ints): Ground truth spans.\n","\n","    Returns:\n","        float: f1 score.\n","    \"\"\"\n","    bin_preds = []\n","    bin_truths = []\n","    for pred, truth in zip(preds, truths):\n","        if not len(pred) and not len(truth):\n","            continue\n","        length = max(np.max(pred) if len(pred) else 0, np.max(truth) if len(truth) else 0)\n","        bin_preds.append(spans_to_binary(pred, length))\n","        bin_truths.append(spans_to_binary(truth, length))\n","        \n","    return micro_f1(bin_preds, bin_truths)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EKbCXKOW7-zx"},"outputs":[],"source":["def create_labels_for_scoring(df):\n","    # example: ['0 1', '3 4'] -> ['0 1; 3 4']\n","    df['location_for_create_labels'] = [ast.literal_eval(f'[]')] * len(df)\n","    for i in range(len(df)):\n","        lst = df.loc[i, 'location']\n","        if lst:\n","            new_lst = ';'.join(lst)\n","            df.loc[i, 'location_for_create_labels'] = ast.literal_eval(f'[[\"{new_lst}\"]]')\n","    # create labels\n","    truths = []\n","    for location_list in df['location_for_create_labels'].values:\n","        truth = []\n","        if len(location_list) > 0:\n","            location = location_list[0]\n","            for loc in [s.split() for s in location.split(';')]:\n","                start, end = int(loc[0]), int(loc[1])\n","                truth.append([start, end])\n","        truths.append(truth)\n","    return truths\n","\n","\n","def get_char_probs(texts, predictions, tokenizer):\n","    results = [np.zeros(len(t)) for t in texts]\n","    for i, (text, prediction) in enumerate(zip(texts, predictions)):\n","        encoded = tokenizer(text, \n","                            add_special_tokens=True,\n","                            return_offsets_mapping=True)\n","        for idx, (offset_mapping, pred) in enumerate(zip(encoded['offset_mapping'], prediction)):\n","            start = offset_mapping[0]\n","            end = offset_mapping[1]\n","            results[i][start:end] = pred\n","    return results\n","\n","\n","def get_results(char_probs, th=0.5):\n","    results = []\n","    for char_prob in char_probs:\n","        result = np.where(char_prob >= th)[0] + 1\n","        result = [list(g) for _, g in itertools.groupby(result, key=lambda n, c=itertools.count(): n - next(c))]\n","        result = [f\"{min(r)} {max(r)}\" for r in result]\n","        result = \";\".join(result)\n","        results.append(result)\n","    return results\n","\n","\n","def get_predictions(results):\n","    predictions = []\n","    for result in results:\n","        prediction = []\n","        if result != \"\":\n","            for loc in [s.split() for s in result.split(';')]:\n","                start, end = int(loc[0]), int(loc[1])\n","                prediction.append([start, end])\n","        predictions.append(prediction)\n","    return predictions\n","\n","def get_score(y_true, y_pred):\n","    score = span_micro_f1(y_true, y_pred)\n","    return score"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LHr7nC327-zx"},"outputs":[],"source":["class ScoringModel(nn.Module):\n","    def __init__(self, cfg, config_path=None, pretrained=False):\n","        super().__init__()\n","        self.cfg = cfg\n","        \n","        if config_path is None:\n","            self.config = AutoConfig.from_pretrained(cfg.model, output_hidden_states=True)\n","        else:\n","            self.config = torch.load(config_path)\n","        if pretrained:\n","            self.model = AutoModel.from_pretrained(cfg.model, config=self.config)\n","        else:\n","            self.model = AutoModel.from_config(self.config)\n","        self.fc_dropout = nn.Dropout(cfg.fc_dropout)\n","        self.fc = nn.Linear(self.config.hidden_size, 1)\n","        self._init_weights(self.fc)\n","        \n","    def _init_weights(self, module):\n","        if isinstance(module, nn.Linear):\n","            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n","            if module.bias is not None:\n","                module.bias.data.zero_()\n","        elif isinstance(module, nn.Embedding):\n","            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n","            if module.padding_idx is not None:\n","                module.weight.data[module.padding_idx].zero_()\n","        elif isinstance(module, nn.LayerNorm):\n","            module.bias.data.zero_()\n","            module.weight.data.fill_(1.0)\n","        \n","    def feature(self, inputs):\n","        outputs = self.model(**inputs)\n","        last_hidden_states = outputs[0]\n","        \n","        return last_hidden_states\n","\n","    def forward(self, inputs):\n","        feature = self.feature(inputs)\n","        output = self.fc(self.fc_dropout(feature))\n","        \n","        return output"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"h8lOm2ke7-zy"},"outputs":[],"source":["CFG.max_len = 466"]},{"cell_type":"code","source":["# mx = []\n","# for i, j in zip(train['pn_history'].values.tolist(), train['feature_text'].values.tolist()):\n","#     inputs = CFG.tokenizer(i, j, \n","#                            add_special_tokens=True,\n","#                            return_offsets_mapping=False)\n","#     mx.append(len(inputs['input_ids']))\n","# np.max(mx)"],"metadata":{"id":"4hxt-ZFhiXLW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZJfHfbJ5jVCR","executionInfo":{"status":"ok","timestamp":1649125846766,"user_tz":-330,"elapsed":8,"user":{"displayName":"Sandeep pandey","userId":"05472944172049299897"}},"outputId":"a19e167c-6b34-4b7d-f3e5-7d13477c77e4"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["335"]},"metadata":{},"execution_count":21}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SjoiJlY37-zy"},"outputs":[],"source":["def prepare_input(cfg, text, feature_text):\n","    inputs = cfg.tokenizer(text, feature_text, \n","                           add_special_tokens=True,\n","                           max_length=CFG.max_len,\n","                           padding=\"max_length\",\n","                           return_offsets_mapping=False)\n","    for k, v in inputs.items():\n","        inputs[k] = torch.tensor(v, dtype=torch.long)\n","    return inputs\n","\n","\n","def create_label(cfg, text, annotation_length, location_list):\n","    encoded = cfg.tokenizer(text,\n","                            add_special_tokens=True,\n","                            max_length=CFG.max_len,\n","                            padding=\"max_length\",\n","                            return_offsets_mapping=True)\n","    offset_mapping = encoded['offset_mapping']\n","    ignore_idxes = np.where(np.array(encoded.sequence_ids()) != 0)[0]\n","    label = np.zeros(len(offset_mapping))\n","    label[ignore_idxes] = -1\n","    if annotation_length != 0:\n","        for location in location_list:\n","            for loc in [s.split() for s in location.split(';')]:\n","                start_idx = -1\n","                end_idx = -1\n","                start, end = int(loc[0]), int(loc[1])\n","                for idx in range(len(offset_mapping)):\n","                    if (start_idx == -1) & (start < offset_mapping[idx][0]):\n","                        start_idx = idx - 1\n","                    if (end_idx == -1) & (end <= offset_mapping[idx][1]):\n","                        end_idx = idx + 1\n","                if start_idx == -1:\n","                    start_idx = end_idx\n","                if (start_idx != -1) & (end_idx != -1):\n","                    label[start_idx:end_idx] = 1\n","    return torch.tensor(label, dtype=torch.float)\n","\n","\n","class TrainDataset(Dataset):\n","    def __init__(self, cfg, df):\n","        self.cfg = cfg\n","        self.feature_texts = df['feature_text'].values\n","        self.pn_historys = df['pn_history'].values\n","        self.annotation_lengths = df['annotation_length'].values\n","        self.locations = df['location'].values\n","\n","    def __len__(self):\n","        return len(self.feature_texts)\n","\n","    def __getitem__(self, item):\n","        inputs = prepare_input(self.cfg, \n","                               self.pn_historys[item], \n","                               self.feature_texts[item])\n","        label = create_label(self.cfg, \n","                             self.pn_historys[item], \n","                             self.annotation_lengths[item], \n","                             self.locations[item])\n","        return inputs, label"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vJPB8P-H7-zz"},"outputs":[],"source":["def seed_everything(seed=42):\n","    random.seed(seed)\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.backends.cudnn.deterministic = True\n","    \n","seed_everything(seed=42)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PedYPewk7-zz"},"outputs":[],"source":["class AverageMeter(object):\n","    \"\"\"Computes and stores the average and current value\"\"\"\n","    def __init__(self):\n","        self.reset()\n","\n","    def reset(self):\n","        self.val = 0\n","        self.avg = 0\n","        self.sum = 0\n","        self.count = 0\n","\n","    def update(self, val, n=1):\n","        self.val = val\n","        self.sum += val * n\n","        self.count += n\n","        self.avg = self.sum / self.count\n","\n","\n","def asMinutes(s):\n","    m = math.floor(s / 60)\n","    s -= m * 60\n","    return '%dm %ds' % (m, s)\n","\n","\n","def timeSince(since, percent):\n","    now = time.time()\n","    s = now - since\n","    es = s / (percent)\n","    rs = es - s\n","    return '%s (remain %s)' % (asMinutes(s), asMinutes(rs))\n","\n","\n","def train_fn(fold, train_loader, model, criterion, optimizer, epoch, scheduler, device):\n","    model.train()\n","    scaler = torch.cuda.amp.GradScaler(enabled=CFG.apex)\n","    losses = AverageMeter()\n","    start = end = time.time()\n","    global_step = 0\n","    for step, (inputs, labels) in enumerate(train_loader):\n","        for k, v in inputs.items():\n","            inputs[k] = v.to(device)\n","        labels = labels.to(device)\n","        batch_size = labels.size(0)\n","        with torch.cuda.amp.autocast(enabled=CFG.apex):\n","            y_preds = model(inputs)\n","        loss = criterion(y_preds.view(-1, 1), labels.view(-1, 1))\n","        loss = torch.masked_select(loss, labels.view(-1, 1) != -1).mean()\n","        if CFG.gradient_accumulation_steps > 1:\n","            loss = loss / CFG.gradient_accumulation_steps\n","        losses.update(loss.item(), batch_size)\n","        scaler.scale(loss).backward()\n","        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), CFG.max_grad_norm)\n","        if (step + 1) % CFG.gradient_accumulation_steps == 0:\n","            scaler.step(optimizer)\n","            scaler.update()\n","            optimizer.zero_grad()\n","            global_step += 1\n","            if CFG.batch_scheduler:\n","                scheduler.step()\n","        end = time.time()\n","        if step % CFG.print_freq == 0 or step == (len(train_loader)-1):\n","            print('Epoch: [{0}][{1}/{2}] '\n","                  'Elapsed {remain:s} '\n","                  'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n","                  'Grad: {grad_norm:.4f}  '\n","                  'LR: {lr:.8f}  '\n","                  .format(epoch+1, step, len(train_loader), \n","                          remain=timeSince(start, float(step+1)/len(train_loader)),\n","                          loss=losses,\n","                          grad_norm=grad_norm,\n","                          lr=scheduler.get_lr()[0]))\n","    return losses.avg\n","\n","\n","def valid_fn(valid_loader, model, criterion, device):\n","    losses = AverageMeter()\n","    model.eval()\n","    preds = []\n","    start = end = time.time()\n","    for step, (inputs, labels) in enumerate(valid_loader):\n","        for k, v in inputs.items():\n","            inputs[k] = v.to(device)\n","        labels = labels.to(device)\n","        batch_size = labels.size(0)\n","        with torch.no_grad():\n","            y_preds = model(inputs)\n","        loss = criterion(y_preds.view(-1, 1), labels.view(-1, 1))\n","        loss = torch.masked_select(loss, labels.view(-1, 1) != -1).mean()\n","        if CFG.gradient_accumulation_steps > 1:\n","            loss = loss / CFG.gradient_accumulation_steps\n","        losses.update(loss.item(), batch_size)\n","        preds.append(y_preds.sigmoid().to('cpu').numpy())\n","        end = time.time()\n","        if step % CFG.print_freq == 0 or step == (len(valid_loader)-1):\n","            print('EVAL: [{0}/{1}] '\n","                  'Elapsed {remain:s} '\n","                  'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n","                  .format(step, len(valid_loader),\n","                          loss=losses,\n","                          remain=timeSince(start, float(step+1)/len(valid_loader))))\n","    predictions = np.concatenate(preds)\n","    return losses.avg, predictions\n","\n","\n","def inference_fn(test_loader, model, device):\n","    preds = []\n","    model.eval()\n","    model.to(device)\n","    tk0 = tqdm(test_loader, total=len(test_loader))\n","    for inputs in tk0:\n","        for k, v in inputs.items():\n","            inputs[k] = v.to(device)\n","        with torch.no_grad():\n","            y_preds = model(inputs)\n","        preds.append(y_preds.sigmoid().to('cpu').numpy())\n","    predictions = np.concatenate(preds)\n","    return predictions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"C4oANu217-z0"},"outputs":[],"source":["def train_loop(folds, fold):\n","    \n","    print(f\"========== fold: {fold} training ==========\")\n","\n","    # ====================================================\n","    # loader\n","    # ====================================================\n","    train_folds = folds[folds['fold'] != fold].reset_index(drop=True)\n","    valid_folds = folds[folds['fold'] == fold].reset_index(drop=True)\n","    valid_texts = valid_folds['pn_history'].values\n","    valid_labels = create_labels_for_scoring(valid_folds)\n","    \n","    train_dataset = TrainDataset(CFG, train_folds)\n","    valid_dataset = TrainDataset(CFG, valid_folds)\n","\n","    train_loader = DataLoader(train_dataset,\n","                              batch_size=CFG.batch_size,\n","                              shuffle=True,\n","                              num_workers=CFG.num_workers, pin_memory=True, drop_last=True)\n","    valid_loader = DataLoader(valid_dataset,\n","                              batch_size=CFG.batch_size,\n","                              shuffle=False,\n","                              num_workers=CFG.num_workers, pin_memory=True, drop_last=False)\n","\n","    # ====================================================\n","    # model & optimizer\n","    # ====================================================\n","    model = ScoringModel(CFG, config_path=None, pretrained=True)\n","    torch.save(model.config, root_dir+\"first_mod/\"+'config.pth')\n","    model.to(device)\n","    \n","    def get_optimizer_params(model, encoder_lr, decoder_lr, weight_decay=0.0):\n","        param_optimizer = list(model.named_parameters())\n","        no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n","        optimizer_parameters = [\n","            {'params': [p for n, p in model.model.named_parameters() if not any(nd in n for nd in no_decay)],\n","             'lr': encoder_lr, 'weight_decay': weight_decay},\n","            {'params': [p for n, p in model.model.named_parameters() if any(nd in n for nd in no_decay)],\n","             'lr': encoder_lr, 'weight_decay': 0.0},\n","            {'params': [p for n, p in model.named_parameters() if \"model\" not in n],\n","             'lr': decoder_lr, 'weight_decay': 0.0}\n","        ]\n","        return optimizer_parameters\n","\n","    optimizer_parameters = get_optimizer_params(model,\n","                                                encoder_lr=CFG.encoder_lr, \n","                                                decoder_lr=CFG.decoder_lr,\n","                                                weight_decay=CFG.weight_decay)\n","    optimizer = AdamW(optimizer_parameters, lr=CFG.encoder_lr, eps=CFG.eps, betas=CFG.betas)\n","    \n","    # ====================================================\n","    # scheduler\n","    # ====================================================\n","    def get_scheduler(cfg, optimizer, num_train_steps):\n","        if cfg.scheduler=='linear':\n","            scheduler = get_linear_schedule_with_warmup(\n","                optimizer, num_warmup_steps=cfg.num_warmup_steps, num_training_steps=num_train_steps\n","            )\n","        elif cfg.scheduler=='cosine':\n","            scheduler = get_cosine_schedule_with_warmup(\n","                optimizer, num_warmup_steps=cfg.num_warmup_steps, num_training_steps=num_train_steps, num_cycles=cfg.num_cycles\n","            )\n","        return scheduler\n","    \n","    num_train_steps = int(len(train_folds) / CFG.batch_size * CFG.epochs)\n","    scheduler = get_scheduler(CFG, optimizer, num_train_steps)\n","\n","    # ====================================================\n","    # loop\n","    # ====================================================\n","    criterion = nn.BCEWithLogitsLoss(reduction=\"none\")\n","    \n","    best_score = 0.\n","\n","    for epoch in range(CFG.epochs):\n","\n","        start_time = time.time()\n","\n","        # train\n","        avg_loss = train_fn(fold, train_loader, model, criterion, optimizer, epoch, scheduler, device)\n","\n","        # eval\n","        avg_val_loss, predictions = valid_fn(valid_loader, model, criterion, device)\n","        predictions = predictions.reshape((len(valid_folds), CFG.max_len))\n","        \n","        # scoring\n","        char_probs = get_char_probs(valid_texts, predictions, CFG.tokenizer)\n","        results = get_results(char_probs, th=0.5)\n","        preds = get_predictions(results)\n","        score = get_score(valid_labels, preds)\n","\n","        elapsed = time.time() - start_time\n","        print(f'Epoch {epoch+1} - avg_train_loss: {avg_loss:.4f}  avg_val_loss: {avg_val_loss:.4f}  time: {elapsed:.0f}s')\n","        print(f'Epoch {epoch+1} - Score: {score:.4f}')\n","\n","        if best_score < score:\n","            best_score = score\n","            print(f'Epoch {epoch+1} - Save Best Score: {best_score:.4f} Model')\n","            torch.save({'model': model.state_dict()},\n","                        root_dir+\"first_mod/\"+f\"{CFG.model.replace('/', '-')}_fold{fold}_best.pth\")\n","\n","    valid_folds[[i for i in range(CFG.max_len)]] = predictions\n","\n","    torch.cuda.empty_cache()\n","    gc.collect()\n","    \n","    return valid_folds"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MYbBuVfV7-z1"},"outputs":[],"source":["def get_result(oof_df):\n","    labels = create_labels_for_scoring(oof_df)\n","    predictions = oof_df[[i for i in range(CFG.max_len)]].values\n","    char_probs = get_char_probs(oof_df['pn_history'].values, predictions, CFG.tokenizer)\n","    results = get_results(char_probs, th=0.5)\n","    preds = get_predictions(results)\n","    score = get_score(labels, preds)\n","    print(f'Score: {score:<.4f}')\n","\n","if CFG.train:\n","    oof_df = pd.DataFrame()\n","    for fold in range(CFG.n_fold):\n","        if fold in CFG.trn_fold:\n","            _oof_df = train_loop(train, fold)\n","            oof_df = pd.concat([oof_df, _oof_df])\n","            print(f\"========== fold: {fold} result ==========\")\n","            get_result(_oof_df)\n","    oof_df = oof_df.reset_index(drop=True)\n","    print(f\"========== CV ==========\")\n","    get_result(oof_df)\n","    oof_df.to_pickle(root_dir+'oofs/oof_df.pkl')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2_GOab9e7-z1"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fzuJGt6e7-z1"},"outputs":[],"source":["#0.8811 + 0.8951 + 0.8614 + 0.8874 + 0.8860 # First 450\n","#0.9001 + 0.8996 + 0.8853 + 0.9078 + 0.9028 # second 450"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"deberta-v3-large-train.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"}},"nbformat":4,"nbformat_minor":0}